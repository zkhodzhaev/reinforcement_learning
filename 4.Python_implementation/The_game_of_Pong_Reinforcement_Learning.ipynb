{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a85a00f",
   "metadata": {},
   "source": [
    "# *Explanation of the code*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e15b1",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef88d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eec0da",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- numpy is used for numerical operations.\n",
    "- tensorflow is the deep learning framework used to build and train the neural network.\n",
    "- gym is an open-source library for developing and comparing reinforcement learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081779bb",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b363a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 200  # Number of neurons in the hidden layer.\n",
    "learning_rate = 1e-4  # Learning rate for the optimizer.\n",
    "gamma = 0.99  # Discount factor for reward, used in the reward discounting process.\n",
    "D = 80 * 80  # Input dimensionality (flattened 80x80 grid)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a5726c",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- Hyperparameters are settings that can be adjusted to control the behavior of the neural network and the learning process.\n",
    "- 'H' represents the size of the hidden layer in the neural network.\n",
    "- 'learning_rate' affects how quickly or slowly the neural network learns. A smaller value means slower learning.\n",
    "- 'gamma' is used in calculating the discounted rewards, influencing how much the model cares about immediate vs. future rewards.\n",
    "- 'D' is the size of the input to the neural network, here representing an 80x80 pixel image flattened into a single vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4526626",
   "metadata": {},
   "source": [
    "## Keras Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da5cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(H, activation='relu', input_shape=(D,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc686b6",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- This code defines the neural network model using Keras, a high-level API in TensorFlow.\n",
    "- 'tf.keras.Sequential' creates a linear stack of layers in the neural network.\n",
    "- The first layer is a Dense (fully connected) layer with 'H' neurons and ReLU activation function. It takes input of shape 'D'.\n",
    "- The second layer is a Dense layer with a single neuron and a sigmoid activation function, outputting the probability of taking a certain action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1244938e",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e09a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf497d",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- 'optimizer' is used to update the network weights based on the computed gradients.\n",
    "- 'tf.keras.optimizers.Adam' is an optimizer that uses the Adam algorithm, a popular choice for deep learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675e6d5f",
   "metadata": {},
   "source": [
    "## Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd0c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    # Code for the preprocessing function 'prepro'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd291ad2",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- The 'prepro' function preprocesses the raw image frames from the game environment.\n",
    "- The preprocessing steps typically include cropping irrelevant parts of the image, downsampling, and normalizing pixel values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d8a68d",
   "metadata": {},
   "source": [
    "## Discounted Rewards Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363fc00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    # Code for the function 'discount_rewards' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28c62c0",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- The 'discount_rewards' function computes the discounted rewards over a sequence of rewards.\n",
    "- This function is crucial in reinforcement learning to account for future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af773f7f",
   "metadata": {},
   "source": [
    "## Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32836340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    # Code for the custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789404b3",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- A custom loss function is defined to suit the specific needs of the reinforcement learning task.\n",
    "- This loss function will be used to update the network weights during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d60a9d4",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a059e80f",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- The training loop involves interacting with the game environment, making decisions based on the model's predictions, and updating the model based on the rewards received from the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4bb4d",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54c525c",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f194c8a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269a2739",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df372e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1d011",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ae174",
   "metadata": {},
   "source": [
    "# *Now to compare between pure python and Keras based*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80cdff",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eada287",
   "metadata": {},
   "source": [
    "### Original Code (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc4da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model = {}\n",
    "model['W1'] = np.random.randn(H, D) / np.sqrt(D)  # \"Xavier\" initialization\n",
    "model['W2'] = np.random.randn(H) / np.sqrt(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a2e489",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "- This code manually initializes the weights of a two-layer neural network using Xavier initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6ba524",
   "metadata": {},
   "source": [
    "### Keras Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3da624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(H, activation='relu', input_shape=(D,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6703f80",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "- The Keras implementation replaces manual weight initialization with a high-level abstraction using tf.keras.Sequential, where layers and their initializations are automatically managed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7e803",
   "metadata": {},
   "source": [
    "## Preprocessing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94579a8",
   "metadata": {},
   "source": [
    "### Original Code (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f92b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195]  # crop\n",
    "    I = I[::2, ::2, 0]  # downsample by factor of 2\n",
    "    I[I == 144] = 0  # erase background (background type 1)\n",
    "    I[I == 109] = 0  # erase background (background type 2)\n",
    "    I[I != 0] = 1  # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622b011",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "- This function preprocesses the input images by cropping, downsampling, and normalizing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db912dbf",
   "metadata": {},
   "source": [
    "### Keras Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb561aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    # Code for the preprocessing function 'prepro' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb245d2",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "- The same preprocessing logic is expected to be used, adapted to work within the TensorFlow ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6a563e",
   "metadata": {},
   "source": [
    "## Forward Propagation and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c29a0f0",
   "metadata": {},
   "source": [
    "### Original Code (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255093c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_forward(x):\n",
    "    h = np.dot(model['W1'], x)\n",
    "    h[h < 0] = 0  # ReLU nonlinearity\n",
    "    logp = np.dot(model['W2'], h)\n",
    "    p = sigmoid(logp)\n",
    "    return p, h  # return probability of taking action 2, and hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dd2d42",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "- Manually conducts forward propagation through the network and uses the sigmoid function to calculate the probability of taking an action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb35fca7",
   "metadata": {},
   "source": [
    "### Keras Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aprob = model.predict(x.reshape(1, -1), batch_size=1).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0aeb82",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "- Uses the predict method of the Keras model to compute the action probability, leveraging TensorFlow's optimized operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81187611",
   "metadata": {},
   "source": [
    "## Training Loop and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca7a6b",
   "metadata": {},
   "source": [
    "### Original Code (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effb6603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy backward pass\n",
    "def policy_backward(eph, epdlogp):\n",
    "    # Code for the backward pass\n",
    "\n",
    "# Training loop with manual backpropagation and weight updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8d8e9",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "- The original code manually calculates gradients and updates weights using backpropagation algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033e7aa",
   "metadata": {},
   "source": [
    "### Keras Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eed596",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    p = model(epx, training=True)\n",
    "    loss = custom_loss(discounted_epr, p)\n",
    "grads = tape.gradient(loss, model.trainable_variables)\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b3bf0",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "- TensorFlow's GradientTape is used for automatic differentiation, and the optimizer handles the weight updates, significantly simplifying the backpropagation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6445cd5",
   "metadata": {},
   "source": [
    "  ## Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56fe0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def custom_loss(advantages, predicted_action_probs):\n",
    "    inverted_action_probs = 1 - predicted_action_probs\n",
    "    loss = -tf.reduce_mean(tf.math.log(predicted_action_probs) * advantages + \n",
    "                           tf.math.log(inverted_action_probs) * (1 - advantages))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7111f8d0",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "#### Purpose: \n",
    "- This function calculates the loss for training the neural network in a reinforcement learning context, specifically for playing Pong.\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "- Advantages: Discounted rewards, indicating how much better (or worse) an action was compared to a baseline.\n",
    "- predicted_action_probs: The probabilities of the actions as predicted by the model.\n",
    "\n",
    "#### Loss Calculation:\n",
    "\n",
    "- inverted_action_probs: Represents the probability of not choosing the action that was actually taken.\n",
    " \n",
    "- The loss is computed as the negative mean of two terms:\n",
    "    - The log probability of the chosen action (predicted_action_probs), multiplied by the advantage. This encourages actions that lead to positive outcomes.\n",
    "    - The log probability of the alternative action (inverted_action_probs), multiplied by the negative of the advantage. This discourages actions that lead to negative outcomes.\n",
    "\n",
    "#### Outcome: \n",
    "- By minimizing this loss, the model is trained to increase the likelihood of actions that yield positive rewards and decrease the likelihood of actions that result in negative rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8531f7e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d1769",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6238cc45",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01829ad",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6535e95c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb491b44",
   "metadata": {},
   "source": [
    "# *And the complete Keras implementation is given as:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246463a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "# Hyperparameters\n",
    "H = 200  # number of hidden layer neurons\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99  # discount factor for reward\n",
    "D = 80 * 80  # input dimensionality: 80x80 grid\n",
    "render = True\n",
    "\n",
    "# Keras Model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(H, activation='relu', input_shape=(D,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Preprocessing function\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    if isinstance(I, tuple):\n",
    "        I = I[0]\n",
    "    I = I[35:195]\n",
    "    I = I[::2, ::2, 0]\n",
    "    I[I == 144] = 0\n",
    "    I[I == 109] = 0\n",
    "    I[I != 0] = 1\n",
    "    return I.astype(float).ravel()\n",
    "\n",
    "# Discounted rewards function\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(r.size)):\n",
    "        if r[t] != 0: running_add = 0\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "# Custom loss function\n",
    "def custom_loss(advantages, predicted_action_probs):\n",
    "    # advantages: The discounted rewards\n",
    "    # predicted_action_probs: The probabilities of the chosen actions from the model\n",
    "\n",
    "    # Inverting the probabilities for actions not taken\n",
    "    inverted_action_probs = 1 - predicted_action_probs\n",
    "\n",
    "    # Combining the probabilities with the advantages\n",
    "    loss = -tf.reduce_mean(tf.math.log(predicted_action_probs) * advantages + tf.math.log(inverted_action_probs) * (1 - advantages))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Training loop\n",
    "env = gym.make(\"Pong-v0\", render_mode='human')\n",
    "observation = env.reset()\n",
    "\n",
    "prev_x = None\n",
    "xs, dlogps, drs = [], [], []\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "\n",
    "running_reward = None \n",
    "\n",
    "while True:\n",
    "    env.render() \n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    prev_x = cur_x\n",
    "\n",
    "    aprob = model.predict(x.reshape(1, -1), batch_size=1).flatten()\n",
    "    action = 2 if np.random.uniform() < aprob else 3\n",
    "\n",
    "    xs.append(x)\n",
    "    y = 1 if action == 2 else 0\n",
    "    dlogps.append(y - aprob)\n",
    "\n",
    "    observation, reward, done, info = env.step(action)[:4]\n",
    "    reward_sum += reward\n",
    "    drs.append(reward)\n",
    "\n",
    "    if done:\n",
    "        episode_number += 1\n",
    "\n",
    "        epx = np.vstack(xs)\n",
    "        epdlogp = np.vstack(dlogps)\n",
    "        epr = np.vstack(drs)\n",
    "        xs, dlogps, drs = [], [], []\n",
    "\n",
    "        discounted_epr = discount_rewards(epr)\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= (np.std(discounted_epr) + 1e-10)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            p = model(epx, training=True)\n",
    "            loss = custom_loss(discounted_epr, p)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        print('Resetting env. Episode reward total was %.f. Running mean: %.f' % (reward_sum, running_reward))\n",
    "        reward_sum = 0\n",
    "        observation = env.reset()\n",
    "        prev_x = None\n",
    "\n",
    "#         if episode_number % 100 == 0:\n",
    "#             model.save('pong_model.h5')\n",
    "        model.save('pong_model.h5')\n",
    "\n",
    "    if reward != 0:\n",
    "        print('Ep %d: Game finished, reward: %f' % (episode_number, reward) + ('' if reward == -1 else ' !!!!!!!!'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08016af",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895ab626",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea2082e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c7343",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff19f3",
   "metadata": {},
   "source": [
    "# *Loading and Running Saved Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe320e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Load the previously trained model\n",
    "model = tf.keras.models.load_model('pong_model_customLoss.h5')\n",
    "\n",
    "# Preprocessing function\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    if isinstance(I, tuple):\n",
    "        I = I[0]\n",
    "    I = I[35:195]\n",
    "    I = I[::2, ::2, 0]\n",
    "    I[I == 144] = 0\n",
    "    I[I == 109] = 0\n",
    "    I[I != 0] = 1\n",
    "    return I.astype(float).ravel()\n",
    "\n",
    "# Initialize the Pong environment\n",
    "env = gym.make(\"Pong-v0\", render_mode='human')\n",
    "observation = env.reset()\n",
    "\n",
    "prev_x = None\n",
    "\n",
    "# Run the model on the environment\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(80 * 80)\n",
    "    prev_x = cur_x\n",
    "\n",
    "    aprob = model.predict(x.reshape(1, -1), batch_size=1).flatten()\n",
    "    action = 2 if np.random.uniform() < aprob else 3\n",
    "\n",
    "    observation, reward, done, info = env.step(action)[:4]\n",
    "\n",
    "    if done:\n",
    "        observation = env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dfa4cc",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30596c1c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5b4530",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09751fef",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
